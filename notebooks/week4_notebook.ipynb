{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Notebook: Data Preprocessing\n",
    "The goal of this week's assignment is to continue to preprocess our data by cleaning it, treating issues such as outliers and missing values, transforming variables, and making the data model-ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Applications/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Applications/anaconda3/lib/python3.11/site-packages (from geopy) (2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### Import packages\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!pip install geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_folder = os.path.join(parent_dir,\"data\")\n",
    "raw_data_folder = os.path.join(data_folder,\"raw\")\n",
    "interim_data_folder = os.path.join(data_folder,\"interim\")\n",
    "processed_data_folder = os.path.join(data_folder, \"processed\")\n",
    "\n",
    "raw_data_file = os.path.join(raw_data_folder, 'rawSampledData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'timestamp', 'hour', 'day', 'month', 'timezone', 'source',\n",
       "       'destination', 'cab_type', 'product_id', 'name', 'price', 'distance',\n",
       "       'surge_multiplier', 'latitude', 'longitude', 'temperature',\n",
       "       'apparentTemperature', 'short_summary', 'long_summary',\n",
       "       'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',\n",
       "       'windGust', 'windGustTime', 'visibility', 'temperatureHigh',\n",
       "       'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',\n",
       "       'apparentTemperatureHigh', 'apparentTemperatureHighTime',\n",
       "       'apparentTemperatureLow', 'apparentTemperatureLowTime', 'icon',\n",
       "       'dewPoint', 'pressure', 'windBearing', 'cloudCover', 'uvIndex',\n",
       "       'visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',\n",
       "       'precipIntensityMax', 'uvIndexTime', 'temperatureMin',\n",
       "       'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',\n",
       "       'apparentTemperatureMin', 'apparentTemperatureMinTime',\n",
       "       'apparentTemperatureMax', 'apparentTemperatureMaxTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df):\n",
    "    # Suffle the dataset and calculate the size of validation and test sets\n",
    "\n",
    "    df = df.sample(frac=1, random_state=123)\n",
    "\n",
    "    val_size = int(len(df) * 0.2)\n",
    "    test_size = int(len(df) * 0.1)\n",
    "\n",
    "    # Select rows based on the val_size and test_size to store as train set, val set, and test set\n",
    "    train_df = df.iloc[val_size + test_size:]\n",
    "    val_df = df.iloc[:val_size]\n",
    "    test_df = df.iloc[val_size:val_size + test_size]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Imputation\n",
    "Uber Taxi Fee Breakdown\n",
    " - base fare: $2.60 for first 1/7 mile\n",
    " - per minute fare: $0.47\n",
    " - per mile: $2.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_price_calculator(distance, time):\n",
    "    base_fare = 2.60\n",
    "    per_min_fare = 0.47\n",
    "    per_mile_fare = 2.8\n",
    "    price = base_fare + distance * per_mile_fare + time * per_min_fare\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have one time stamp rather the duration of each ride. We will need to figure out a way to estimate the time costed for each Taxi ride for a more accurate imputation. Here are the Steps.\n",
    "\n",
    "1. Get the unique records for locations as a lst and save the unique combination of sources as a csv\n",
    "2. Get the longtitude and latitude for these loctaions and save it as a dict with location as the key and value being [lat, long]\n",
    "3. Read the csv and create sourece latitude, soure longtitude, desitination latitude, destination longtitude columns based on the location dict\n",
    "4. Use the Mapbox Direction API to pull time estimated by driving\n",
    "5. Save the csv with source, destination, time_estimated\n",
    "\n",
    "Please refer to the code folder to get the code for how we get the estimated duration for the taxi rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theatre District' 'Fenway' 'Beacon Hill' 'Haymarket Square'\n",
      " 'Northeastern University' 'North Station' 'Back Bay' 'Financial District'\n",
      " 'South Station' 'Boston University' 'North End' 'West End']\n"
     ]
    }
   ],
   "source": [
    "unique_combos = df[[\"source\",\"destination\"]].drop_duplicates()\n",
    "unique_combos_dir = os.path.join(interim_data_folder, \"unique_combo.csv\")\n",
    "unique_combos.to_csv(unique_combos_dir)\n",
    "\n",
    "locations = pd.unique(df[[\"source\",\"destination\"]].values.ravel())\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longtitude_latitude(location):\n",
    "    # calling the Nominatim tool and create Nominatim class\n",
    "    loc = Nominatim(user_agent=\"Geopy Library\")\n",
    "    location = location+\", Boston\"\n",
    "    getLoc = loc.geocode(location)\n",
    "    return getLoc.latitude, getLoc.longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commented out the part below for memory management, but we left the code block to demonstrat how the longtitue and latitude were added to the unique combinations of sources and destinations. The file was later used to get the estimated duration of trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dict = {}\n",
    "\n",
    "for location in locations:\n",
    "    lat, long = get_longtitude_latitude(location)\n",
    "    if lat is not None and long is not None:\n",
    "        location_dict[location] = [lat, long]\n",
    "\n",
    "unique_combos[\"source_lat\"] = unique_combos['source'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"source_long\"] = unique_combos['source'].apply(lambda x: location_dict[x][1])\n",
    "\n",
    "unique_combos[\"destination_lat\"] = unique_combos['destination'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"destination_long\"] = unique_combos['destination'].apply(lambda x: location_dict[x][1])\n",
    "\n",
    "ride_locations_dir = os.path.join(interim_data_folder, \"ride_locations.csv\")\n",
    "unique_combos.to_csv(ride_locations_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `get_eta` function requires api_key, so I saved the dataframe locally for reviewing convinience. Pleaser refer to the `time_calulator.py` under the codes folder for more details how the time was imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>eta_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North End</td>\n",
       "      <td>West End</td>\n",
       "      <td>7.751050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>South Station</td>\n",
       "      <td>10.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North Station</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>13.937950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North End</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>12.600567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston University</td>\n",
       "      <td>North Station</td>\n",
       "      <td>15.743100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source    destination  eta_minutes\n",
       "0          North End       West End     7.751050\n",
       "1        Beacon Hill  South Station    10.896667\n",
       "2      North Station         Fenway    13.937950\n",
       "3          North End    Beacon Hill    12.600567\n",
       "4  Boston University  North Station    15.743100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides_with_eta = os.path.join(interim_data_folder, \"rides_with_etas.csv\")\n",
    "time_df = pd.read_csv(rides_with_eta)\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, time_df[['source', 'destination', 'eta_minutes']], on=['source', 'destination'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>timezone</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>...</th>\n",
       "      <th>uvIndexTime</th>\n",
       "      <th>temperatureMin</th>\n",
       "      <th>temperatureMinTime</th>\n",
       "      <th>temperatureMax</th>\n",
       "      <th>temperatureMaxTime</th>\n",
       "      <th>apparentTemperatureMin</th>\n",
       "      <th>apparentTemperatureMinTime</th>\n",
       "      <th>apparentTemperatureMax</th>\n",
       "      <th>apparentTemperatureMaxTime</th>\n",
       "      <th>eta_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-27 08:06:22</td>\n",
       "      <td>1.543306e+09</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>Financial District</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>lyft</td>\n",
       "      <td>...</td>\n",
       "      <td>1543338000</td>\n",
       "      <td>36.13</td>\n",
       "      <td>1543377600</td>\n",
       "      <td>46.83</td>\n",
       "      <td>1543320000</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1543377600</td>\n",
       "      <td>43.85</td>\n",
       "      <td>1543320000</td>\n",
       "      <td>20.181533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-14 14:30:06</td>\n",
       "      <td>1.544798e+09</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>North End</td>\n",
       "      <td>Back Bay</td>\n",
       "      <td>Uber</td>\n",
       "      <td>6c84fd89-3f11-4782-9b50-97c468b19529</td>\n",
       "      <td>...</td>\n",
       "      <td>1544806800</td>\n",
       "      <td>27.05</td>\n",
       "      <td>1544781600</td>\n",
       "      <td>46.67</td>\n",
       "      <td>1544814000</td>\n",
       "      <td>24.47</td>\n",
       "      <td>1544785200</td>\n",
       "      <td>43.88</td>\n",
       "      <td>1544817600</td>\n",
       "      <td>19.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-01 02:58:02</td>\n",
       "      <td>1.543633e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>South Station</td>\n",
       "      <td>Theatre District</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>lyft_premier</td>\n",
       "      <td>...</td>\n",
       "      <td>1543593600</td>\n",
       "      <td>28.64</td>\n",
       "      <td>1543575600</td>\n",
       "      <td>42.57</td>\n",
       "      <td>1543600800</td>\n",
       "      <td>27.20</td>\n",
       "      <td>1543568400</td>\n",
       "      <td>40.51</td>\n",
       "      <td>1543611600</td>\n",
       "      <td>1789.339967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-03 07:23:05</td>\n",
       "      <td>1.543822e+09</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>North Station</td>\n",
       "      <td>North End</td>\n",
       "      <td>Uber</td>\n",
       "      <td>6c84fd89-3f11-4782-9b50-97c468b19529</td>\n",
       "      <td>...</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>42.96</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>57.87</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>39.41</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>57.20</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>5.562867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-03 22:18:06</td>\n",
       "      <td>1.543875e+09</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>North End</td>\n",
       "      <td>Back Bay</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>lyft_premier</td>\n",
       "      <td>...</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>42.89</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>57.27</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>39.54</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>56.60</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>19.508700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime     timestamp  hour  day  month          timezone  \\\n",
       "0  2018-11-27 08:06:22  1.543306e+09     8   27     11  America/New_York   \n",
       "1  2018-12-14 14:30:06  1.544798e+09    14   14     12  America/New_York   \n",
       "2  2018-12-01 02:58:02  1.543633e+09     2    1     12  America/New_York   \n",
       "3  2018-12-03 07:23:05  1.543822e+09     7    3     12  America/New_York   \n",
       "4  2018-12-03 22:18:06  1.543875e+09    22    3     12  America/New_York   \n",
       "\n",
       "              source         destination cab_type  \\\n",
       "0  Boston University  Financial District     Lyft   \n",
       "1          North End            Back Bay     Uber   \n",
       "2      South Station    Theatre District     Lyft   \n",
       "3      North Station           North End     Uber   \n",
       "4          North End            Back Bay     Lyft   \n",
       "\n",
       "                             product_id  ... uvIndexTime  temperatureMin  \\\n",
       "0                                  lyft  ...  1543338000           36.13   \n",
       "1  6c84fd89-3f11-4782-9b50-97c468b19529  ...  1544806800           27.05   \n",
       "2                          lyft_premier  ...  1543593600           28.64   \n",
       "3  6c84fd89-3f11-4782-9b50-97c468b19529  ...  1543852800           42.96   \n",
       "4                          lyft_premier  ...  1543852800           42.89   \n",
       "\n",
       "   temperatureMinTime  temperatureMax  temperatureMaxTime  \\\n",
       "0          1543377600           46.83          1543320000   \n",
       "1          1544781600           46.67          1544814000   \n",
       "2          1543575600           42.57          1543600800   \n",
       "3          1543896000           57.87          1543852800   \n",
       "4          1543896000           57.27          1543852800   \n",
       "\n",
       "   apparentTemperatureMin  apparentTemperatureMinTime  apparentTemperatureMax  \\\n",
       "0                   32.05                  1543377600                   43.85   \n",
       "1                   24.47                  1544785200                   43.88   \n",
       "2                   27.20                  1543568400                   40.51   \n",
       "3                   39.41                  1543896000                   57.20   \n",
       "4                   39.54                  1543896000                   56.60   \n",
       "\n",
       "  apparentTemperatureMaxTime  eta_minutes  \n",
       "0                 1543320000    20.181533  \n",
       "1                 1544817600    19.508700  \n",
       "2                 1543611600  1789.339967  \n",
       "3                 1543852800     5.562867  \n",
       "4                 1543852800    19.508700  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['name'] == 'Taxi', 'price'] = train_df.loc[train_df['name'] == 'Taxi'].apply(\n",
    "    lambda row: taxi_price_calculator(row['distance'], row['eta_minutes']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                       0\n",
       "timestamp                      0\n",
       "hour                           0\n",
       "day                            0\n",
       "month                          0\n",
       "timezone                       0\n",
       "source                         0\n",
       "destination                    0\n",
       "cab_type                       0\n",
       "product_id                     0\n",
       "name                           0\n",
       "price                          0\n",
       "distance                       0\n",
       "surge_multiplier               0\n",
       "latitude                       0\n",
       "longitude                      0\n",
       "temperature                    0\n",
       "apparentTemperature            0\n",
       "short_summary                  0\n",
       "long_summary                   0\n",
       "precipIntensity                0\n",
       "precipProbability              0\n",
       "humidity                       0\n",
       "windSpeed                      0\n",
       "windGust                       0\n",
       "windGustTime                   0\n",
       "visibility                     0\n",
       "temperatureHigh                0\n",
       "temperatureHighTime            0\n",
       "temperatureLow                 0\n",
       "temperatureLowTime             0\n",
       "apparentTemperatureHigh        0\n",
       "apparentTemperatureHighTime    0\n",
       "apparentTemperatureLow         0\n",
       "apparentTemperatureLowTime     0\n",
       "icon                           0\n",
       "dewPoint                       0\n",
       "pressure                       0\n",
       "windBearing                    0\n",
       "cloudCover                     0\n",
       "uvIndex                        0\n",
       "visibility.1                   0\n",
       "ozone                          0\n",
       "sunriseTime                    0\n",
       "sunsetTime                     0\n",
       "moonPhase                      0\n",
       "precipIntensityMax             0\n",
       "uvIndexTime                    0\n",
       "temperatureMin                 0\n",
       "temperatureMinTime             0\n",
       "temperatureMax                 0\n",
       "temperatureMaxTime             0\n",
       "apparentTemperatureMin         0\n",
       "apparentTemperatureMinTime     0\n",
       "apparentTemperatureMax         0\n",
       "apparentTemperatureMaxTime     0\n",
       "eta_minutes                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi = train_df[train_df[\"name\" ]== \"Taxi\"]\n",
    "df_taxi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('eta_minutes', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in 'price' after imputing: 0.00%\n"
     ]
    }
   ],
   "source": [
    "df_na = train_df[train_df['price'].isna()]\n",
    "missing_percentage_after_imputing =train_df['price'].isna().sum() / len(train_df) * 100\n",
    "print(f\"Percentage of missing values in 'price' after imputing: {missing_percentage_after_imputing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Uneeded Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns that are irrelevant to the modeling because they are just unique identifiers:\n",
    "- `id`\n",
    "- `product_id`\n",
    "\n",
    "In addition, we also dropped `timezone` since all data is within the same timezone.\n",
    "\n",
    "`datetime` and`timestamp` were dropped to reduce redundacy with other time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'timestamp', 'timezone', 'product_id', 'datetime']\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "val_df = val_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hour', 'day', 'month', 'source', 'destination', 'cab_type', 'name',\n",
       "       'price', 'distance', 'surge_multiplier', 'latitude', 'longitude',\n",
       "       'temperature', 'apparentTemperature', 'short_summary', 'long_summary',\n",
       "       'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',\n",
       "       'windGust', 'windGustTime', 'visibility', 'temperatureHigh',\n",
       "       'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',\n",
       "       'apparentTemperatureHigh', 'apparentTemperatureHighTime',\n",
       "       'apparentTemperatureLow', 'apparentTemperatureLowTime', 'icon',\n",
       "       'dewPoint', 'pressure', 'windBearing', 'cloudCover', 'uvIndex',\n",
       "       'visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',\n",
       "       'precipIntensityMax', 'uvIndexTime', 'temperatureMin',\n",
       "       'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',\n",
       "       'apparentTemperatureMin', 'apparentTemperatureMinTime',\n",
       "       'apparentTemperatureMax', 'apparentTemperatureMaxTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Columns:\n",
      "['source', 'destination', 'cab_type', 'name', 'short_summary', 'long_summary', 'icon']\n"
     ]
    }
   ],
   "source": [
    "object_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Display the object columns\n",
    "print(\"Object Columns:\")\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables Encoding\n",
    "### Encoding the `short_summary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Mostly Cloudy ', ' Light Rain ', ' Overcast ', ' Clear ',\n",
       "       ' Partly Cloudy ', ' Rain ', ' Foggy ', ' Drizzle ',\n",
       "       ' Possible Drizzle '], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['short_summary'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The short_summary variable will be divided into binary categories:\n",
    "\n",
    "- Mostly Cloudy\n",
    "- Rain\n",
    "- Clear\n",
    "- Partly Cloudy\n",
    "- Overcast\n",
    "- Light Rain\n",
    "- Foggy\n",
    "- Possible Drizzle\n",
    "- Drizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['short_summary'] = train_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "val_df['short_summary'] = val_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "test_df['short_summary'] = test_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'short_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['short_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['short_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['short_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'short_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `long_summary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Partly cloudy throughout the day. ',\n",
       "       ' Light rain until evening. ',\n",
       "       ' Mostly cloudy throughout the day. ',\n",
       "       ' Rain until morning, starting again in the evening. ',\n",
       "       ' Light rain in the morning. ', ' Overcast throughout the day. ',\n",
       "       ' Foggy in the morning. ',\n",
       "       ' Light rain in the morning and overnight. ',\n",
       "       ' Rain throughout the day. ', ' Possible drizzle in the morning. ',\n",
       "       ' Rain in the morning and afternoon. '], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['long_summary'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long_summary variable will be divided into binary categories:\n",
    "\n",
    "- Rain throughout the day\n",
    "- Rain until morning, starting again in the evening\n",
    "- Light rain in the morning\n",
    "- Partly cloudy thoughout the day\n",
    "- Light rain in the morning and overnight\n",
    "- Light rain until evening\n",
    "- Foggy in the morning\n",
    "- Overcast throughout the day\n",
    "- Possible drizzle in the morning\n",
    "- Rain in the morning and afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['long_summary'] = train_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "val_df['long_summary'] = val_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "test_df['long_summary'] = test_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['long_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['long_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['long_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the icon variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' partly-cloudy-day ', ' rain ', ' cloudy ', ' clear-night ',\n",
       "       ' clear-day ', ' partly-cloudy-night ', ' fog '], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['icon'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for the icon feature will be:\n",
    "\n",
    "- partly cloudy night\n",
    "- rain\n",
    "- clear night\n",
    "- cloudy\n",
    "- fog\n",
    "- clear-day\n",
    "- partly cloudy day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading/trailing spaces and replace blank spaces with underscores in the icon column\n",
    "train_df['icon'] = train_df['icon'].str.strip().str.replace(' ', '_')\n",
    "val_df['icon'] = val_df['icon'].str.strip().str.replace(' ', '_')\n",
    "test_df['icon'] = test_df['icon'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['icon']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['icon']])\n",
    "test_encoded_summary = encoder.transform(test_df[['icon']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding `source` and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for source:\n",
      "['Theatre District' 'Beacon Hill' 'Northeastern University' 'Fenway'\n",
      " 'Back Bay' 'Haymarket Square' 'South Station' 'Financial District'\n",
      " 'Boston University' 'North End' 'North Station' 'West End']\n",
      "Unique values for destination:\n",
      "['Fenway' 'Haymarket Square' 'North Station' 'Back Bay' 'Beacon Hill'\n",
      " 'Financial District' 'Theatre District' 'South Station'\n",
      " 'Boston University' 'West End' 'Northeastern University' 'North End']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for source:\")\n",
    "print(df['source'].unique())\n",
    "\n",
    "print(\"Unique values for destination:\")\n",
    "print(df['destination'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for these two features will be:\n",
    "\n",
    "- Haymarket Square\n",
    "- Back Bay\n",
    "- North End\n",
    "- North Station\n",
    "- Beacon Hill\n",
    "- Boston University\n",
    "- Fenway\n",
    "- South Station\n",
    "- Theatre District\n",
    "- West End\n",
    "- Financial District\n",
    "- Northeastern University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['source'] = train_df['source'].str.strip().str.replace(' ', '_')\n",
    "val_df['source'] = val_df['source'].str.strip().str.replace(' ', '_')\n",
    "test_df['source'] = test_df['source'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'source' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['source']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['source']])\n",
    "test_encoded_summary = encoder.transform(test_df[['source']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'source' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the categories from the fitted OneHotEncoder for 'source'\n",
    "source_categories = encoder.categories_[0]\n",
    "\n",
    "# Initialize a new OneHotEncoder for 'destination'\n",
    "destination_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Strip leading/trailing spaces and replace blank spaces with underscores in the destination column\n",
    "train_df['destination'] = train_df['destination'].str.strip().str.replace(' ', '_')\n",
    "val_df['destination'] = val_df['destination'].str.strip().str.replace(' ', '_')\n",
    "test_df['destination'] = test_df['destination'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Fit the encoder on the training data 'destination' column\n",
    "train_encoded_destination = destination_encoder.fit_transform(train_df[['destination']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_destination = destination_encoder.transform(val_df[['destination']])\n",
    "test_encoded_destination = destination_encoder.transform(test_df[['destination']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_destination_df = pd.DataFrame(train_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "val_encoded_destination_df = pd.DataFrame(val_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "test_encoded_destination_df = pd.DataFrame(test_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'destination' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_destination_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_destination_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_destination_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(\"Train DataFrame:\")\n",
    "# print(train_df.head())\n",
    "# print(\"\\nValidation DataFrame:\")\n",
    "# print(val_df.head())\n",
    "# print(\"\\nTest DataFrame:\")\n",
    "# print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `name` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lyft XL', 'Taxi', 'Lyft', 'Lux Black XL', 'Black SUV', 'UberX',\n",
       "       'UberPool', 'UberXL', 'Lux', 'WAV', 'Shared', 'Lux Black', 'Black'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['name'] = train_df['name'].str.strip().str.replace(' ', '_')\n",
    "val_df['name'] = val_df['name'].str.strip().str.replace(' ', '_')\n",
    "test_df['name'] = test_df['name'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'name' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['name']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['name']])\n",
    "test_encoded_summary = encoder.transform(test_df[['name']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'name' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `cab_type` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lyft', 'Uber'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cab_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cab_type'] = train_df['cab_type'].str.strip()\n",
    "val_df['cab_type'] = val_df['cab_type'].str.strip()\n",
    "test_df['cab_type'] = test_df['cab_type'].str.strip()\n",
    "\n",
    "# Ensure there are no leading or trailing spaces in the 'cab_type' column\n",
    "train_df['cab_type'] = train_df['cab_type'].str.strip()\n",
    "val_df['cab_type'] = val_df['cab_type'].str.strip()\n",
    "test_df['cab_type'] = test_df['cab_type'].str.strip()\n",
    "\n",
    "# Create dummy variables for the 'cab_type' column\n",
    "train_df = pd.get_dummies(train_df, columns=['cab_type'], drop_first=True)\n",
    "val_df = pd.get_dummies(val_df, columns=['cab_type'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['cab_type'], drop_first=True)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_to_drop = ['source',\n",
    " 'destination',\n",
    " 'name',\n",
    " 'short_summary',\n",
    " 'long_summary',\n",
    " 'icon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = cat_columns_to_drop)\n",
    "val_df = val_df.drop(columns = cat_columns_to_drop)\n",
    "test_df = test_df.drop(columns = cat_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Information using .info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70000 entries, 0 to 69999\n",
      "Columns: 110 entries, hour to cab_type_Uber\n",
      "dtypes: bool(1), float64(92), int64(17)\n",
      "memory usage: 58.3 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataFrame Information using .info():\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization and PCA\n",
    "The kernel kept dying at this point, so we cleared the memories and commented out the code we used to standardize the features. Instead, we imported the standardized files generated by the code blocks for memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[['price']]\n",
    "y_val = val_df[['price']]\n",
    "y_test = test_df[['price']]\n",
    "\n",
    "X_train = train_df.drop('price', axis = 1)\n",
    "X_val = val_df.drop('price', axis = 1)\n",
    "X_test = test_df.drop('price', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaled DataFrames as Parquet files\n",
    "\n",
    "X_train_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_train_scaled.parquet'), index=False)\n",
    "X_val_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_val_scaled.parquet'), index=False)\n",
    "X_test_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_test_scaled.parquet'), index=False)\n",
    "\n",
    "# Save the target variables (if needed)\n",
    "y_train.to_parquet(os.path.join(processed_data_folder, 'y_train.parquet'), index=False)\n",
    "y_val.to_parquet(os.path.join(processed_data_folder, 'y_val.parquet'), index=False)\n",
    "y_test.to_parquet(os.path.join(processed_data_folder, 'y_test.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA, you can adjust n_components as needed (for example, n_components=0.95 for 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "\n",
    "X_train_scaled = X_train_scaled_df.to_numpy()\n",
    "X_val_scaled = X_val_scaled_df.to_numpy()\n",
    "X_test_scaled = X_test_scaled_df.to_numpy()\n",
    "\n",
    "# Fit PCA on the scaled training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the validation and test sets using the same PCA\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Convert the PCA results back to DataFrames for easier handling\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "X_val_pca_df = pd.DataFrame(X_val_pca)\n",
    "X_test_pca_df = pd.DataFrame(X_test_pca)\n",
    "\n",
    "# Optional: Rename the columns for clarity\n",
    "X_train_pca_df.columns = [f'PC{i+1}' for i in range(X_train_pca_df.shape[1])]\n",
    "X_val_pca_df.columns = [f'PC{i+1}' for i in range(X_val_pca_df.shape[1])]\n",
    "X_test_pca_df.columns = [f'PC{i+1}' for i in range(X_test_pca_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 56)\n",
      "(20000, 56)\n",
      "(10000, 56)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pca_df.shape)\n",
    "print(X_val_pca_df.shape)\n",
    "print(X_test_pca_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduces the df to 56 components, which is very useful for models like linear regression which would suffer from high-dimentional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA DataFrames as Parquet files\n",
    "train_pca_path = os.path.join(processed_data_folder, 'X_train_pca.parquet')\n",
    "val_pca_path = os.path.join(processed_data_folder, 'X_val_pca.parquet')\n",
    "test_pca_path = os.path.join(processed_data_folder, 'X_test_pca.parquet')\n",
    "\n",
    "X_train_pca_df.to_parquet(train_pca_path, index=False)\n",
    "X_val_pca_df.to_parquet(val_pca_path, index=False)\n",
    "X_test_pca_df.to_parquet(test_pca_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
