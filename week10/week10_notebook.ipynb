{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Notebook: Apply Data-Centric AI Principles\n",
    "The goal of this week's assignment is to apply data-centric AI principles to your modeling project. Keep the chosen model constant and iterate over the data to improve your prediction performance.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import fastparquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "model_folder = os.path.join(parent_dir,\"models\")\n",
    "data_folder = os.path.join(parent_dir,\"data\")\n",
    "\n",
    "raw_data_folder = os.path.join(data_folder,\"raw\")\n",
    "interim_data_folder = os.path.join(data_folder,\"interim\")\n",
    "processed_data_folder = os.path.join(data_folder, \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for feature variables\n",
    "X_train_scaled_path = os.path.join(processed_data_folder, 'X_train_scaled.parquet')\n",
    "X_val_scaled_path = os.path.join(processed_data_folder, 'X_val_scaled.parquet')\n",
    "X_test_scaled_path = os.path.join(processed_data_folder, 'X_test_scaled.parquet')\n",
    "\n",
    "train_pca_path = os.path.join(processed_data_folder, 'X_train_pca.parquet')\n",
    "val_pca_path = os.path.join(processed_data_folder, 'X_val_pca.parquet')\n",
    "test_pca_path = os.path.join(processed_data_folder, 'X_test_pca.parquet')\n",
    "\n",
    "X_train_poly_path = os.path.join(processed_data_folder, \"X_train_poly.parquet\")\n",
    "X_val_poly_path = os.path.join(processed_data_folder, \"X_val_poly.parquet\")\n",
    "\n",
    "# Paths for the target variables\n",
    "y_train_path = os.path.join(processed_data_folder, 'y_train.parquet')\n",
    "y_val_path = os.path.join(processed_data_folder, 'y_val.parquet')\n",
    "y_test_path = os.path.join(processed_data_folder, 'y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the parquet files as dataframes\n",
    "X_train_scaled = pd.read_parquet(X_train_scaled_path)\n",
    "X_val_scaled = pd.read_parquet(X_val_scaled_path)\n",
    "X_test_scaled = pd.read_parquet(X_test_scaled_path)\n",
    "\n",
    "y_train = pd.read_parquet(y_train_path)\n",
    "y_val = pd.read_parquet(y_val_path)\n",
    "y_test = pd.read_parquet(y_test_path)\n",
    "\n",
    "X_train_pca = pd.read_parquet(train_pca_path)\n",
    "X_val_pca = pd.read_parquet(val_pca_path)\n",
    "X_test_pca = pd.read_parquet(test_pca_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models: {'l1_model': Lasso(alpha=0.1), 'l2_model': Ridge(alpha=100), 'ols_model': LinearRegression(), 'ols_pca_model': LinearRegression(), 'rf1': RandomForestRegressor(max_depth=15, max_features=None, random_state=42), 'rf2': RandomForestRegressor(max_depth=15, max_features=None, n_estimators=200,\n",
      "                      random_state=42), 'rf3': RandomForestRegressor(max_depth=10, max_features=None, n_estimators=200,\n",
      "                      random_state=42), 'rt1': DecisionTreeRegressor(), 'rt2': DecisionTreeRegressor(max_depth=15, min_samples_leaf=10, min_samples_split=10), 'rt3': DecisionTreeRegressor(ccp_alpha=0.001, max_depth=20, min_samples_leaf=10)}\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "model_names = []\n",
    "for file_name in os.listdir(model_folder):\n",
    "    if file_name.endswith('.pkl'):  # Check for pickle files\n",
    "        model_name = file_name[:-4]  # Remove the .pkl extension\n",
    "        model_names.append(model_name)\n",
    "        file_path = os.path.join(model_folder, file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            models[model_name] = pickle.load(file)  # Load the model\n",
    "\n",
    "print(\"Loaded models:\", models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function used to predict and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mse = rmse**2\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_data(model, model_name, actual, X_scaled, X_pca):\n",
    "    if \"pca\" in model_name.lower():  # Use .lower() to ensure case-insensitive matching\n",
    "        y_pred = model.predict(X_pca)\n",
    "    else:\n",
    "        y_pred = model.predict(X_scaled)\n",
    "\n",
    "    mse, rmse, r2 = evaluate_model(actual, y_pred)\n",
    "\n",
    "    return mse, rmse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate Models on the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Model Name: l1_model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eta_minutes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m models[model_name]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Use the predict_on_data function for validation data\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m mse, rmse, r2 \u001b[38;5;241m=\u001b[39m predict_on_data(model, model_name, y_val, X_val_scaled, X_val_pca)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Append the calculated metrics to their respective lists\u001b[39;00m\n\u001b[0;32m     16\u001b[0m val_mse\u001b[38;5;241m.\u001b[39mappend(mse)\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36mpredict_on_data\u001b[1;34m(model, model_name, actual, X_scaled, X_pca)\u001b[0m\n\u001b[0;32m      3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_pca)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_scaled)\n\u001b[0;32m      7\u001b[0m mse, rmse, r2 \u001b[38;5;241m=\u001b[39m evaluate_model(actual, y_pred)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mse, rmse, r2\n",
      "File \u001b[1;32mc:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:306\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1147\u001b[0m, in \u001b[0;36mElasticNet._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:285\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    283\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    286\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coef_\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\holly\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eta_minutes\n"
     ]
    }
   ],
   "source": [
    "val_mse = []\n",
    "val_rmse = []\n",
    "val_r2 = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    model_name = model_names[i]\n",
    "    print(f\"Index: {i}, Model Name: {model_name}\")\n",
    "    \n",
    "    # Get the model using the model name\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Use the predict_on_data function for validation data\n",
    "    mse, rmse, r2 = predict_on_data(model, model_name, y_val, X_val_scaled, X_val_pca)\n",
    "    \n",
    "    # Append the calculated metrics to their respective lists\n",
    "    val_mse.append(mse)\n",
    "    val_rmse.append(rmse)\n",
    "    val_r2.append(r2)\n",
    "\n",
    "# Check if all metrics have been collected\n",
    "print(\"MSE List Length:\", len(val_mse))\n",
    "print(\"RMSE List Length:\", len(val_rmse))\n",
    "print(\"R2 List Length:\", len(val_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = []\n",
    "train_rmse = []\n",
    "train_r2 = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    model_name = model_names[i]\n",
    "    print(f\"Index: {i}, Model Name: {model_name}\")\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Use the predict_on_data function for training data\n",
    "    mse, rmse, r2 = predict_on_data(model, model_name, y_train, X_train_scaled, X_train_pca)\n",
    "    \n",
    "    # Append the results to the appropriate lists\n",
    "    train_mse.append(mse)\n",
    "    train_rmse.append(rmse)\n",
    "    train_r2.append(r2)\n",
    "\n",
    "# Check if all metrics have been collected\n",
    "print(\"MSE List Length:\", len(train_mse))\n",
    "print(\"RMSE List Length:\", len(train_rmse))\n",
    "print(\"R2 List Length:\", len(train_r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the winning model\n",
    "### Bias and Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the model based on its complexity\n",
    "model_complexity = {\n",
    "    \"ols_model\": 1,\n",
    "    \"ols_pca_model\": 2,\n",
    "    \"l1_model\": 3,\n",
    "    \"l2_model\": 4,\n",
    "    \"rt1\": 5,\n",
    "    \"rt2\": 6,\n",
    "    \"rt3\": 7,\n",
    "    \"rf1\": 9,\n",
    "    \"rf2\": 10,\n",
    "    \"rf3\": 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'model_name': model_names,\n",
    "    'train_MSE': train_mse,\n",
    "    'train_RMSE': train_rmse,\n",
    "    'train_R2': train_r2,\n",
    "    'val_MSE': val_mse,\n",
    "    'val_RMSE': val_rmse,\n",
    "    'val_R2': val_r2\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"model_complexity\"] = df[\"model_name\"].map(model_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by model complexity for plotting\n",
    "df = df.sort_values('model_complexity')\n",
    "\n",
    "# Plot the Training and Validation MSE vs. Model Complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['model_complexity'], df['train_MSE'], marker='o', linestyle='-', label='Train MSE')\n",
    "plt.plot(df['model_complexity'], df['val_MSE'], marker='o', linestyle='-', label='Validation MSE')\n",
    "\n",
    "plt.xticks(df['model_complexity'], df['model_name'], rotation=45)\n",
    "plt.xlabel('Model Complexity (Simplest to Most Complex)')\n",
    "plt.ylabel('MSE (Mean Squared Error)')\n",
    "plt.title('Bias-Variance Tradeoff: Model Complexity vs MSE')\n",
    "plt.legend()\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big gap between the linear regression models and the tree-based model. To better assess which model performs the best in terms of accuracy and generalization. We want to zoom in the graph for the tree-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for model complexity greater than 4\n",
    "df_filtered = df[df['model_complexity'] > 4]\n",
    "\n",
    "# Plot the Training and Validation MSE for the filtered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_filtered['model_complexity'], df_filtered['train_MSE'], marker='o', linestyle='-', label='Train MSE')\n",
    "plt.plot(df_filtered['model_complexity'], df_filtered['val_MSE'], marker='o', linestyle='-', label='Validation MSE')\n",
    "\n",
    "plt.xticks(df_filtered['model_complexity'], df_filtered['model_name'], rotation=45)\n",
    "plt.xlabel('Model Complexity (Simplest to Most Complex)')\n",
    "plt.ylabel('MSE (Mean Squared Error)')\n",
    "plt.title('Bias-Variance Tradeoff: Model Complexity vs MSE (Filtered)')\n",
    "plt.legend()\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected regressoin tree 3, which is a pruned decision tree, as the winning model. The pruned decision tree shows a smaller gap between training and validation RMSE, indicating low variance. In addition, it also has lower biases in comparison to another strong model random forest 3. The Bias and Varaince Chart showed that the regression tree 3 has a good balance with lower variance and a solid generalization capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winning model performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_mode = models['rt3']\n",
    "y_train_pred_tree = winning_mode.predict(X_train_scaled)\n",
    "y_test_pred_tree = winning_mode.predict(X_test_scaled)\n",
    "y_val_pred_tree = winning_mode.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "train_mse_tree, train_rmse_tree, train_r2_tree = evaluate_model(y_train, y_train_pred_tree)\n",
    "val_mse_tree, val_rmse_tree, val_r2_tree = evaluate_model(y_val, y_val_pred_tree)\n",
    "test_mse_tree, test_rmse_tree, test_r2_tree = evaluate_model(y_test, y_test_pred_tree)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Decision Tree Regression Model - Training Metrics:\")\n",
    "print(f\"MSE: {train_mse_tree:.4f}, RMSE: {train_rmse_tree:.4f}, R²: {train_r2_tree:.4f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")\n",
    "\n",
    "# Print testing metrics\n",
    "print(\"\\nDecision Tree Regression Model - Testing Metrics:\") \n",
    "print(f\"MSE: {test_mse_tree:.4f}, RMSE: {test_rmse_tree:.4f}, R²: {test_r2_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned Decision Tree Model Error Analysis (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis for pruned decision tree model on training data\n",
    "y_train_pred_tree = models['rt3'].predict(X_train_scaled)\n",
    "train_errors = y_train - y_train_pred_tree\n",
    "\n",
    "# Visualize the distribution of errors\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(train_errors, bins=30, kde=True)\n",
    "plt.title(\"Distribution of Training Errors\")\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Further insights: Analyze errors by feature\n",
    "feature_errors = X_train_scaled.copy()\n",
    "feature_errors['error'] = train_errors\n",
    "sns.pairplot(feature_errors, y_vars='error', x_vars=feature_errors.columns[:-1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
