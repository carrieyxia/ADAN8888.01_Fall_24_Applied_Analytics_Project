{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Notebook: Model training, hyperparameter tuning, and model evaluation\n",
    "The goal of this week's assignment is to use a third modeling method with 3 different hyperparameter settings of the method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_folder = os.path.join(parent_dir,\"data\")\n",
    "raw_data_folder = os.path.join(data_folder,\"raw\")\n",
    "interim_data_folder = os.path.join(data_folder,\"interim\")\n",
    "processed_data_folder = os.path.join(data_folder, \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data path\n",
    "X_train_scaled_path = os.path.join(processed_data_folder, 'X_train_scaled.parquet')\n",
    "X_val_scaled_path = os.path.join(processed_data_folder, 'X_val_scaled.parquet')\n",
    "X_test_scaled_path = os.path.join(processed_data_folder, 'X_test_scaled.parquet')\n",
    "\n",
    "train_pca_path = os.path.join(processed_data_folder, 'X_train_pca.parquet')\n",
    "val_pca_path = os.path.join(processed_data_folder, 'X_val_pca.parquet')\n",
    "test_pca_path = os.path.join(processed_data_folder, 'X_test_pca.parquet')\n",
    "# Y data path\n",
    "y_train_path = os.path.join(processed_data_folder, 'y_train.parquet')\n",
    "y_val_path = os.path.join(processed_data_folder, 'y_val.parquet')\n",
    "y_test_path = os.path.join(processed_data_folder, 'y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the parquet files as dataframes\n",
    "X_train_scaled = pd.read_parquet(X_train_scaled_path)\n",
    "X_val_scaled = pd.read_parquet(X_val_scaled_path)\n",
    "X_test_scaled = pd.read_parquet(X_test_scaled_path)\n",
    "\n",
    "y_train = pd.read_parquet(y_train_path)\n",
    "y_val = pd.read_parquet(y_val_path)\n",
    "y_test = pd.read_parquet(y_test_path)\n",
    "\n",
    "X_train_pca = pd.read_parquet(train_pca_path)\n",
    "X_val_pca = pd.read_parquet(val_pca_path)\n",
    "X_test_pca = pd.read_parquet(test_pca_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Modeling\n",
    "#### Ravel y into numpy array\n",
    "Sklearn random forest requires y to be an 1D array, so we need to ravel the panda dataframe to a 1D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.ravel()\n",
    "y_val = y_val.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mse = rmse**2\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Define Function for Random Forest Hyperparameter Tuning\n",
    "Random Forest can be computationally intensive, making cross-validation difficult when tuning hyperparameters. To streamline the process of experimenting with multiple hyperparameters, we created a function that automates the training and evaluation steps on the validation set, helping us identify the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_rf(X_train, X_val, y_train, y_val, param_combinations, lowest_rmse = float('inf')):\n",
    "    for params in param_combinations:\n",
    "    # Initialize the model with the current parameters\n",
    "        rf_model = RandomForestRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        max_features=params['max_features'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "        rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "        y_val_pred = rf_model.predict(X_val)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "        rmse = root_mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "        if rmse < lowest_rmse:\n",
    "            lowest_rmse = rmse\n",
    "            best_model = rf_model\n",
    "            best_params = params\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Lowest RMSE on validation set:\", lowest_rmse)\n",
    "    return best_model, best_params, lowest_rmse\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning `n_estimators`\n",
    "`n_estimators` refers number of decision trees in the random forest. We first want to experiment how that affect the model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 10, 'max_features': 'log2'}\n",
      "Lowest RMSE on validation set: 60.647192316432616\n"
     ]
    }
   ],
   "source": [
    "param_combinations = [\n",
    "    {'n_estimators': 50, 'max_depth': 10, 'max_features':'log2'},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'max_features': 'log2'},\n",
    "    {'n_estimators': 150, 'max_depth': 10, 'max_features': 'log2'}\n",
    "]\n",
    "\n",
    "best_model, best_params, lowest_rmse = best_rf(X_train_scaled, X_val_scaled, y_train, y_val, param_combinations,lowest_rmse = float('inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning `max_depth`\n",
    "`max_depth` controls the maximum depth of each decision tree in the random forest. We will experiment with this parameter to understand its impact on model performance and find an optimal value that balances complexity and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 15, 'max_features': 'log2'}\n",
      "Lowest RMSE on validation set: 41.15554705419018\n"
     ]
    }
   ],
   "source": [
    "param_combinations = [\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'max_features': 'log2'},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'max_features': 'log2'},\n",
    "    {'n_estimators': 100, 'max_depth': 15, 'max_features': 'log2'}\n",
    "]\n",
    "best_model, best_params, lowest_rmse = best_rf(X_train_scaled, X_val_scaled, y_train, y_val, param_combinations, lowest_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning `max_features`\n",
    "`max_features` determines the maximum number of features considered when splitting a node in each decision tree. We will experiment with different values to see how this parameter affects model performance, aiming to find a balance between diversity in the trees and overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 15, 'max_features': None}\n",
      "Lowest RMSE on validation set: 1.739659226065485\n"
     ]
    }
   ],
   "source": [
    "param_combinations = [\n",
    "    {'n_estimators': 100, 'max_depth': 15, 'max_features': None},\n",
    "    {'n_estimators': 100, 'max_depth': 15, 'max_features': 'log2'},\n",
    "    {'n_estimators': 100, 'max_depth': 15, 'max_features': 'sqrt'}\n",
    "]\n",
    "best_model, best_params, lowest_rmse = best_rf(X_train_scaled, X_val_scaled, y_train, y_val, param_combinations, lowest_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random Forest 1\n",
    "We evaluate the best model based on RMSE from exploring different values for the three hyperparameters. While the model does perform a lot better, the performance metrics shows that the model is overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_pred_tree \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_test_pred_tree \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[1;32m      3\u001b[0m y_val_pred_tree \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_val_scaled)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:1076\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m-> 1076\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m y_hat \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:731\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[1;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/sklearn/tree/_classes.py:530\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    528\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    529\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[0;32m--> 530\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train_pred_tree = best_model.predict(X_train_scaled)\n",
    "y_test_pred_tree = best_model.predict(X_test_scaled)\n",
    "y_val_pred_tree = best_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "train_mse_tree, train_rmse_tree, train_r2_tree = evaluate_model(y_train, y_train_pred_tree)\n",
    "val_mse_tree, val_rmse_tree, val_r2_tree = evaluate_model(y_val, y_val_pred_tree)\n",
    "test_mse_tree, test_rmse_tree, test_r2_tree = evaluate_model(y_test, y_test_pred_tree)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Decision Tree Regression Model - Training Metrics:\")\n",
    "print(f\"MSE: {train_mse_tree:.4f}, RMSE: {train_rmse_tree:.4f}, R²: {train_r2_tree:.4f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")\n",
    "\n",
    "# Print testing metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest 2\n",
    "For the second random forest varaition, I want to experiment if `max_features` = n_features, more estimators will lead to better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_combinations = [\n",
    "    {'n_estimators': 150, 'max_depth': 15, 'max_features': None},\n",
    "    {'n_estimators': 200, 'max_depth': 15, 'max_features': None},\n",
    "    {'n_estimators': 250, 'max_depth': 15, 'max_features': None}\n",
    "]\n",
    "best_model, best_params, lowest_rmse = best_rf(X_train_scaled, X_val_scaled, y_train, y_val, param_combinations, lowest_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Random Forest 2\n",
    "Similarly, the model shows slightly improved performance but still overfits the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = best_model.predict(X_train_scaled)\n",
    "y_test_pred_tree = best_model.predict(X_test_scaled)\n",
    "y_val_pred_tree = best_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "train_mse_tree, train_rmse_tree, train_r2_tree = evaluate_model(y_train, y_train_pred_tree)\n",
    "val_mse_tree, val_rmse_tree, val_r2_tree = evaluate_model(y_val, y_val_pred_tree)\n",
    "test_mse_tree, test_rmse_tree, test_r2_tree = evaluate_model(y_test, y_test_pred_tree)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Decision Tree Regression Model - Training Metrics:\")\n",
    "print(f\"MSE: {train_mse_tree:.4f}, RMSE: {train_rmse_tree:.4f}, R²: {train_r2_tree:.4f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")\n",
    "\n",
    "# Print testing metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest 3\n",
    "#### Addressing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_combinations = [\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'max_features': None},\n",
    "    {'n_estimators': 150, 'max_depth': 10, 'max_features': None},\n",
    "    {'n_estimators': 200, 'max_depth': 10, 'max_features': None},\n",
    "]\n",
    "best_model, best_params, lowest_rmse = best_rf(X_train_scaled, X_val_scaled, y_train, y_val, param_combinations, lowest_rmse= float('inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random Forest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = best_model.predict(X_train_scaled)\n",
    "y_test_pred_tree = best_model.predict(X_test_scaled)\n",
    "y_val_pred_tree = best_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "train_mse_tree, train_rmse_tree, train_r2_tree = evaluate_model(y_train, y_train_pred_tree)\n",
    "val_mse_tree, val_rmse_tree, val_r2_tree = evaluate_model(y_val, y_val_pred_tree)\n",
    "test_mse_tree, test_rmse_tree, test_r2_tree = evaluate_model(y_test, y_test_pred_tree)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Decision Tree Regression Model - Training Metrics:\")\n",
    "print(f\"MSE: {train_mse_tree:.4f}, RMSE: {train_rmse_tree:.4f}, R²: {train_r2_tree:.4f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")\n",
    "\n",
    "# Print testing metrics\n",
    "print(\"\\nDecision Tree Regression Model - Validation Metrics:\") \n",
    "print(f\"MSE: {val_mse_tree:.4f}, RMSE: {val_rmse_tree:.4f}, R²: {val_r2_tree:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
