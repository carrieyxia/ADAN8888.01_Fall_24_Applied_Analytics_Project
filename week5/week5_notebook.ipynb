{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Notebook: Engineer Features\n",
    "The goal of this week's assignment is to engineer new features and reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_folder = os.path.join(parent_dir,\"data\")\n",
    "raw_data_folder = os.path.join(data_folder,\"raw\")\n",
    "\n",
    "uber_file_path = os.path.join(raw_data_folder, \"uber.csv.zip\")\n",
    "lyft_file_path = os.path.join(raw_data_folder, \"lyft.csv.zip\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if os.path.exists(uber_file_path):\n",
    "    with zipfile.ZipFile(uber_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(raw_data_folder)\n",
    "    print(f\"Uber file extracted to: {raw_data_folder}\")\n",
    "else:\n",
    "    print(f\"Uber file not found: {uber_file_path}\")\n",
    "\n",
    "if os.path.exists(lyft_file_path):\n",
    "    with zipfile.ZipFile(lyft_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(raw_data_folder)\n",
    "    print(f\"Lyft file extracted to: {raw_data_folder}\")\n",
    "else:\n",
    "    print(f\"Lyft file not found: {lyft_file_path}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "uber_csv_path = os.path.join(raw_data_folder, \"uber.csv\")\n",
    "lyft_csv_path = os.path.join(raw_data_folder, \"lyft.csv\")\n",
    "\n",
    "uber_df = pd.read_csv(uber_csv_path)\n",
    "lyft_df = pd.read_csv(lyft_csv_path)\n",
    "    \n",
    "df = pd.concat([uber_df, lyft_df], ignore_index=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'timestamp', 'hour', 'day', 'month', 'timezone', 'source',\n",
      "       'destination', 'cab_type', 'product_id', 'name', 'price', 'distance',\n",
      "       'surge_multiplier', 'latitude', 'longitude', 'temperature',\n",
      "       'apparentTemperature', 'short_summary', 'long_summary',\n",
      "       'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',\n",
      "       'windGust', 'windGustTime', 'visibility', 'temperatureHigh',\n",
      "       'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',\n",
      "       'apparentTemperatureHigh', 'apparentTemperatureHighTime',\n",
      "       'apparentTemperatureLow', 'apparentTemperatureLowTime', 'icon',\n",
      "       'dewPoint', 'pressure', 'windBearing', 'cloudCover', 'uvIndex',\n",
      "       'visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',\n",
      "       'precipIntensityMax', 'uvIndexTime', 'temperatureMin',\n",
      "       'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',\n",
      "       'apparentTemperatureMin', 'apparentTemperatureMinTime',\n",
      "       'apparentTemperatureMax', 'apparentTemperatureMaxTime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the data as a pandas dataframe.\n",
    "df = pd.read_csv('rawSampledData.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training, testing, and validation sets\n",
    "- training set is 70% of the dataframe\n",
    "- validation set is 20% of the dataframe\n",
    "- test set is 10% of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df):\n",
    "    # Shuffle the dataset and calculate the size of validation and test sets\n",
    "\n",
    "    df = df.sample(frac=1, random_state=123)\n",
    "\n",
    "    val_size = int(len(df) * 0.2)\n",
    "    test_size = int(len(df) * 0.1)\n",
    "\n",
    "    # Select rows based on the val_size and test_size to store as train set, val set, and test set\n",
    "    train_df = df.iloc[val_size + test_size:]\n",
    "    val_df = df.iloc[:val_size]\n",
    "    test_df = df.iloc[val_size:val_size + test_size]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = train_val_test_split(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_col = 9\n",
      "num_col = 46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_col = [col for col in df.columns if df[col].dtype == 'object']\n",
    "cat_col = [\n",
    " 'timezone',\n",
    " 'source',\n",
    " 'destination',\n",
    " 'cab_type',\n",
    " 'product_id',\n",
    " 'name',\n",
    " 'short_summary',\n",
    " 'long_summary',\n",
    " 'icon']\n",
    "num_col = [col for col in df.columns if col not in cat_col]\n",
    "\n",
    "# Remove 'id' and 'datetime' from the list of column names\n",
    "num_col = [col for col in num_col if col not in ['id', 'datetime']]\n",
    "\n",
    "print(f\"cat_col = {len(cat_col)}\\nnum_col = {len(num_col)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timestamp',\n",
       " 'hour',\n",
       " 'day',\n",
       " 'month',\n",
       " 'price',\n",
       " 'distance',\n",
       " 'surge_multiplier',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'temperature',\n",
       " 'apparentTemperature',\n",
       " 'precipIntensity',\n",
       " 'precipProbability',\n",
       " 'humidity',\n",
       " 'windSpeed',\n",
       " 'windGust',\n",
       " 'windGustTime',\n",
       " 'visibility',\n",
       " 'temperatureHigh',\n",
       " 'temperatureHighTime',\n",
       " 'temperatureLow',\n",
       " 'temperatureLowTime',\n",
       " 'apparentTemperatureHigh',\n",
       " 'apparentTemperatureHighTime',\n",
       " 'apparentTemperatureLow',\n",
       " 'apparentTemperatureLowTime',\n",
       " 'dewPoint',\n",
       " 'pressure',\n",
       " 'windBearing',\n",
       " 'cloudCover',\n",
       " 'uvIndex',\n",
       " 'visibility.1',\n",
       " 'ozone',\n",
       " 'sunriseTime',\n",
       " 'sunsetTime',\n",
       " 'moonPhase',\n",
       " 'precipIntensityMax',\n",
       " 'uvIndexTime',\n",
       " 'temperatureMin',\n",
       " 'temperatureMinTime',\n",
       " 'temperatureMax',\n",
       " 'temperatureMaxTime',\n",
       " 'apparentTemperatureMin',\n",
       " 'apparentTemperatureMinTime',\n",
       " 'apparentTemperatureMax',\n",
       " 'apparentTemperatureMaxTime']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_price_calculator(distance, time):\n",
    "    base_fare = 2.60\n",
    "    per_min_fare = 0.47\n",
    "    per_mile_fare = 2.8\n",
    "    price = base_fare + distance * per_mile_fare + time * per_min_fare\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theatre District' 'Fenway' 'Beacon Hill' 'Haymarket Square'\n",
      " 'Northeastern University' 'North Station' 'Back Bay' 'Financial District'\n",
      " 'South Station' 'Boston University' 'North End' 'West End']\n"
     ]
    }
   ],
   "source": [
    "unique_combos = df[[\"source\",\"destination\"]].drop_duplicates()\n",
    "unique_combos.to_csv(\"unique_combo.csv\")\n",
    "\n",
    "locations = pd.unique(df[[\"source\",\"destination\"]].values.ravel())\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mget_long_lat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_longtitude_latitude\n\u001b[0;32m      3\u001b[0m location_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m location \u001b[38;5;129;01min\u001b[39;00m locations:\n",
      "File \u001b[1;32mc:\\Users\\holly\\OneDrive\\Documents\\GitHub\\ADAN8888.01_Fall_24_Applied_Analytics_Project\\week5\\get_long_lat.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nominatim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_longtitude_latitude\u001b[39m(location):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# calling the Nominatim tool and create Nominatim class\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "from get_long_lat import get_longtitude_latitude\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for location in locations:\n",
    "    lat, long = get_longtitude_latitude(location)\n",
    "    if lat is not None and long is not None:\n",
    "        location_dict[location] = [lat, long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combos[\"source_lat\"] = unique_combos['source'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"source_long\"] = unique_combos['source'].apply(lambda x: location_dict[x][1])\n",
    "\n",
    "unique_combos[\"destination_lat\"] = unique_combos['destination'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"destination_long\"] = unique_combos['destination'].apply(lambda x: location_dict[x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_with_eta = os.path.join(saved_data_folder, \"rides_with_etas.csv\")\n",
    "time_df = pd.read_csv(rides_with_eta)\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, time_df[['source', 'destination', 'eta_minutes']], on=['source', 'destination'], how = 'left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['name'] == 'Taxi', 'price'] = df.loc[df['name'] == 'Taxi'].apply(\n",
    "    lambda row: taxi_price_calculator(row['distance'], row['eta_minutes']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi = df[df[\"name\" ]== \"Taxi\"]\n",
    "df_taxi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('eta_minutes', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df[df['price'].isna()]\n",
    "missing_percentage_after_imputing = df['price'].isna().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values in 'price' after imputing: {missing_percentage_after_imputing:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the prices have been correctly imputed\n",
    "df_taxi = df[df[\"name\"] == \"Taxi\"]\n",
    "df_taxi[['distance', 'price']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values after imputing\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df[df['price'].isna()]\n",
    "missing_percentage_after_imputing = df['price'].isna().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values in 'price' after imputing: {missing_percentage_after_imputing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **One-hot Encoding Categorical Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding the short_summary variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lists all of the unique values in the short_summary\n",
    "print(df['short_summary'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short_summary variable will be divided into binary categories: \n",
    "* Mostly Cloudy\n",
    "* Rain\n",
    "* Clear\n",
    "* Partly Cloudy\n",
    "* Overcast\n",
    "* Light Rain\n",
    "* Foggy\n",
    "* Possible Drizzle\n",
    "* Drizzle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Ensure there are no leading or trailing spaces in the 'short_summary' column \n",
    "train_df['short_summary'] = train_df['short_summary'].str.strip()\n",
    "val_df['short_summary'] = val_df['short_summary'].str.strip()\n",
    "test_df['short_summary'] = test_df['short_summary'].str.strip()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'short_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['short_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['short_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['short_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'short_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# Drop the original 'short_summary' column from each dataset\n",
    "train_df.drop('short_summary', axis=1, inplace=True)\n",
    "val_df.drop('short_summary', axis=1, inplace=True)\n",
    "test_df.drop('short_summary', axis=1, inplace=True)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding the long_summary variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all of the unique values in the long_summary\n",
    "print(df['long_summary'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long_summary variable will be divided into binary categories:\n",
    "\n",
    "* Rain throughout the day\n",
    "* Rain until morning, starting again in the evening \n",
    "* Light rain in the morning\n",
    "* Partly cloudy thoughout the day\n",
    "* Light rain in the morning and overnight\n",
    "* Light rain until evening \n",
    "* Foggy in the morning\n",
    "* Overcast throughout the day\n",
    "* Possible drizzle in the morning\n",
    "* Rain in the morning and afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Ensure there are no leading or trailing spaces in the 'long_summary' column \n",
    "train_df['long_summary'] = train_df['long_summary'].str.strip()\n",
    "val_df['long_summary'] = val_df['long_summary'].str.strip()\n",
    "test_df['long_summary'] = test_df['long_summary'].str.strip()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['long_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['long_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['long_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# Drop the original 'long_summary' column from each dataset\n",
    "train_df.drop('long_summary', axis=1, inplace=True)\n",
    "val_df.drop('long_summary', axis=1, inplace=True)\n",
    "test_df.drop('long_summary', axis=1, inplace=True)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding the icon variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists all of the unique icon values\n",
    "print(df['icon'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for the icon feature will be:\n",
    "* partly cloudy night\n",
    "* rain\n",
    "* clear night\n",
    "* cloudy\n",
    "* fog\n",
    "* clear-day\n",
    "* partly cloudy day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure there are no leading or trailing spaces in the 'icon' column \n",
    "train_df['icon'] = train_df['icon'].str.strip()\n",
    "val_df['icon'] = val_df['icon'].str.strip()\n",
    "test_df['icon'] = test_df['icon'].str.strip()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['icon']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['icon']])\n",
    "test_encoded_summary = encoder.transform(test_df[['icon']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# Drop the original 'long_summary' column from each dataset\n",
    "train_df.drop('icon', axis=1, inplace=True)\n",
    "val_df.drop('icon', axis=1, inplace=True)\n",
    "test_df.drop('icon', axis=1, inplace=True)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The timezone variable**\n",
    "The timezone variable is America/New_York for every ride in the dataset so we decided to drop it as it does not provide any useful information distinguishing between data points and will therefore add no additional value to our analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists all of the unique timezone values\n",
    "print(df['timezone'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'timezone' column after splitting\n",
    "train_df = train_df.drop('timezone', axis=1)\n",
    "val_df = val_df.drop('timezone', axis=1)\n",
    "test_df = test_df.drop('timezone', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['source'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for the source feature will be:\n",
    "* Haymarket Square\n",
    "* Back Bay\n",
    "* North End\n",
    "* North Station\n",
    "* Beacon Hill \n",
    "* Boston University\n",
    "* Fenway\n",
    "* South Station\n",
    "* Theatre District\n",
    "* West End\n",
    "* Financial District\n",
    "* Northeastern University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no leading or trailing spaces in the 'source' column \n",
    "train_df['source'] = train_df['source'].str.strip()\n",
    "val_df['source'] = val_df['source'].str.strip()\n",
    "test_df['source'] = test_df['source'].str.strip()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'source' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['source']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['source']])\n",
    "test_encoded_summary = encoder.transform(test_df[['source']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'source' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# Drop the original 'source' column from each dataset\n",
    "train_df.drop('source', axis=1, inplace=True)\n",
    "val_df.drop('source', axis=1, inplace=True)\n",
    "test_df.drop('source', axis=1, inplace=True)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **The name variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for the name feature will be:\n",
    "* Shared\n",
    "* Lux\n",
    "* Lyft\n",
    "* Lux Black XL\n",
    "* Lyft XL\n",
    "* Lux Black\n",
    "* UberXL\n",
    "* Black\n",
    "* UberX\n",
    "* WAV\n",
    "* Black SUV\n",
    "* UberPool\n",
    "* Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no leading or trailing spaces in the 'name' column \n",
    "train_df['name'] = train_df['name'].str.strip()\n",
    "val_df['name'] = val_df['name'].str.strip()\n",
    "test_df['name'] = test_df['name'].str.strip()\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'name' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['name']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['name']])\n",
    "test_encoded_summary = encoder.transform(test_df[['name']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'name' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# Drop the original 'name' column from each dataset\n",
    "train_df.drop('name', axis=1, inplace=True)\n",
    "val_df.drop('name', axis=1, inplace=True)\n",
    "test_df.drop('name', axis=1, inplace=True)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
