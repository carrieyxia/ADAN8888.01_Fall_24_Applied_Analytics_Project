{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Notebook: Data Preprocessing\n",
    "The goal of this week's assignment is to continue to preprocess our data by cleaning it, treating issues such as outliers and missing values, transforming variables, and making the data model-ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_folder = os.path.join(parent_dir,\"data\")\n",
    "raw_data_folder = os.path.join(data_folder,\"raw\")\n",
    "interim_data_folder = os.path.join(data_folder,\"interim\")\n",
    "\n",
    "uber_file_path = os.path.join(raw_data_folder, \"uber.csv.zip\")\n",
    "lyft_file_path = os.path.join(raw_data_folder, \"lyft.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber file extracted to: /Users/carriexia/Documents/GitHub/ADAN8888.01_Fall_24_Applied_Analytics_Project/data/raw\n",
      "Lyft file extracted to: /Users/carriexia/Documents/GitHub/ADAN8888.01_Fall_24_Applied_Analytics_Project/data/raw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists(uber_file_path):\n",
    "    with zipfile.ZipFile(uber_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(raw_data_folder)\n",
    "    print(f\"Uber file extracted to: {raw_data_folder}\")\n",
    "else:\n",
    "    print(f\"Uber file not found: {uber_file_path}\")\n",
    "\n",
    "if os.path.exists(lyft_file_path):\n",
    "    with zipfile.ZipFile(lyft_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(raw_data_folder)\n",
    "    print(f\"Lyft file extracted to: {raw_data_folder}\")\n",
    "else:\n",
    "    print(f\"Lyft file not found: {lyft_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_csv_path = os.path.join(raw_data_folder, \"uber.csv\")\n",
    "lyft_csv_path = os.path.join(raw_data_folder, \"lyft.csv\")\n",
    "\n",
    "uber_df = pd.read_csv(uber_csv_path)\n",
    "lyft_df = pd.read_csv(lyft_csv_path)\n",
    "    \n",
    "df = pd.concat([uber_df, lyft_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df):\n",
    "    # Suffle the dataset and calculate the size of validation and test sets\n",
    "\n",
    "    df = df.sample(frac=1, random_state=123)\n",
    "\n",
    "    val_size = int(len(df) * 0.2)\n",
    "    test_size = int(len(df) * 0.1)\n",
    "\n",
    "    # Select rows based on the val_size and test_size to store as train set, val set, and test set\n",
    "    train_df = df.iloc[val_size + test_size:]\n",
    "    val_df = df.iloc[:val_size]\n",
    "    test_df = df.iloc[val_size:val_size + test_size]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Imputation\n",
    "Uber Taxi Fee Breakdown\n",
    " - base fare: $2.60 for first 1/7 mile\n",
    " - per minute fare: $0.47\n",
    " - per mile: $2.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_price_calculator(distance, time):\n",
    "    base_fare = 2.60\n",
    "    per_min_fare = 0.47\n",
    "    per_mile_fare = 2.8\n",
    "    price = base_fare + distance * per_mile_fare + time * per_min_fare\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have one time stamp rather the duration of each ride. We will need to figure out a way to estimate the time costed for each Taxi ride for a more accurate imputation. Here are the Steps.\n",
    "\n",
    "1. Get the unique records for locations as a lst and save the unique combination of sources as a csv\n",
    "2. Get the longtitude and latitude for these loctaions and save it as a dict with location as the key and value being [lat, long]\n",
    "3. Read the csv and create sourece latitude, soure longtitude, desitination latitude, destination longtitude columns based on the location dict\n",
    "4. Use the Mapbox Direction API to pull time estimated by driving\n",
    "5. Save the csv with source, destination, time_estimated\n",
    "\n",
    "Please refer to the code folder to get the code for how we get the estimated duration for the taxi rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['North End' 'West End' 'Beacon Hill' 'South Station' 'North Station'\n",
      " 'Fenway' 'Boston University' 'Back Bay' 'Theatre District'\n",
      " 'Northeastern University' 'Financial District' 'Haymarket Square']\n"
     ]
    }
   ],
   "source": [
    "unique_combos = df[[\"source\",\"destination\"]].drop_duplicates()\n",
    "unique_combos_dir = os.path.join(interim_data_folder, \"unique_combo.csv\")\n",
    "unique_combos.to_csv(unique_combos_dir)\n",
    "\n",
    "locations = pd.unique(df[[\"source\",\"destination\"]].values.ravel())\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longtitude_latitude(location):\n",
    "    # calling the Nominatim tool and create Nominatim class\n",
    "    loc = Nominatim(user_agent=\"Geopy Library\")\n",
    "    location = location+\", Boston\"\n",
    "    getLoc = loc.geocode(location)\n",
    "    return getLoc.latitude, getLoc.longitude\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for location in locations:\n",
    "    lat, long = get_longtitude_latitude(location)\n",
    "    if lat is not None and long is not None:\n",
    "        location_dict[location] = [lat, long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combos[\"source_lat\"] = unique_combos['source'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"source_long\"] = unique_combos['source'].apply(lambda x: location_dict[x][1])\n",
    "\n",
    "unique_combos[\"destination_lat\"] = unique_combos['destination'].apply(lambda x: location_dict[x][0])\n",
    "unique_combos[\"destination_long\"] = unique_combos['destination'].apply(lambda x: location_dict[x][1])\n",
    "\n",
    "ride_locations_dir = os.path.join(interim_data_folder, \"ride_locations.csv\")\n",
    "unique_combos.to_csv(ride_locations_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `get_eta` function requires api_key, so I saved the dataframe locally for reviewing convinience. Pleaser refer to the `time_calulator.py` under the codes folder for more details how the time was imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>eta_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North End</td>\n",
       "      <td>West End</td>\n",
       "      <td>7.751050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>South Station</td>\n",
       "      <td>10.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North Station</td>\n",
       "      <td>Fenway</td>\n",
       "      <td>13.937950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North End</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>12.600567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston University</td>\n",
       "      <td>North Station</td>\n",
       "      <td>15.743100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source    destination  eta_minutes\n",
       "0          North End       West End     7.751050\n",
       "1        Beacon Hill  South Station    10.896667\n",
       "2      North Station         Fenway    13.937950\n",
       "3          North End    Beacon Hill    12.600567\n",
       "4  Boston University  North Station    15.743100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides_with_eta = os.path.join(interim_data_folder, \"rides_with_etas.csv\")\n",
    "time_df = pd.read_csv(rides_with_eta)\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, time_df[['source', 'destination', 'eta_minutes']], on=['source', 'destination'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>datetime</th>\n",
       "      <th>timezone</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>cab_type</th>\n",
       "      <th>...</th>\n",
       "      <th>uvIndexTime</th>\n",
       "      <th>temperatureMin</th>\n",
       "      <th>temperatureMinTime</th>\n",
       "      <th>temperatureMax</th>\n",
       "      <th>temperatureMaxTime</th>\n",
       "      <th>apparentTemperatureMin</th>\n",
       "      <th>apparentTemperatureMinTime</th>\n",
       "      <th>apparentTemperatureMax</th>\n",
       "      <th>apparentTemperatureMaxTime</th>\n",
       "      <th>eta_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d134c416-199e-485a-a1bb-ce4a9c21881c</td>\n",
       "      <td>1.545123e+09</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-12-18 08:55:08</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>North End</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>...</td>\n",
       "      <td>1545152400</td>\n",
       "      <td>23.23</td>\n",
       "      <td>1545192000</td>\n",
       "      <td>38.11</td>\n",
       "      <td>1545109200</td>\n",
       "      <td>11.86</td>\n",
       "      <td>1545134400</td>\n",
       "      <td>31.96</td>\n",
       "      <td>1545109200</td>\n",
       "      <td>6.628483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>379e231f-6c29-4223-90c9-35ce5aa294c9</td>\n",
       "      <td>1.543528e+09</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-11-29 21:48:07</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>South Station</td>\n",
       "      <td>Financial District</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>...</td>\n",
       "      <td>1543510800</td>\n",
       "      <td>35.12</td>\n",
       "      <td>1543550400</td>\n",
       "      <td>44.76</td>\n",
       "      <td>1543510800</td>\n",
       "      <td>30.85</td>\n",
       "      <td>1543550400</td>\n",
       "      <td>38.44</td>\n",
       "      <td>1543510800</td>\n",
       "      <td>3.833850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c53e90ed-eeb4-4a64-8290-67d2c83302a2</td>\n",
       "      <td>1.543282e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-11-27 01:30:22</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>Northeastern University</td>\n",
       "      <td>Beacon Hill</td>\n",
       "      <td>Uber</td>\n",
       "      <td>...</td>\n",
       "      <td>1543251600</td>\n",
       "      <td>40.49</td>\n",
       "      <td>1543233600</td>\n",
       "      <td>47.30</td>\n",
       "      <td>1543251600</td>\n",
       "      <td>36.20</td>\n",
       "      <td>1543291200</td>\n",
       "      <td>43.92</td>\n",
       "      <td>1543251600</td>\n",
       "      <td>12.323333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6542a54b-836e-4960-925e-0dabf6c1af92</td>\n",
       "      <td>1.543832e+09</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-12-03 10:08:00</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>North End</td>\n",
       "      <td>North Station</td>\n",
       "      <td>Uber</td>\n",
       "      <td>...</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>42.96</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>57.87</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>39.41</td>\n",
       "      <td>1543896000</td>\n",
       "      <td>57.20</td>\n",
       "      <td>1543852800</td>\n",
       "      <td>6.628483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25caa475-c3da-4770-a729-9625c192827b</td>\n",
       "      <td>1.543621e+09</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-11-30 23:33:01</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>Financial District</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>Uber</td>\n",
       "      <td>...</td>\n",
       "      <td>1543593600</td>\n",
       "      <td>28.90</td>\n",
       "      <td>1543579200</td>\n",
       "      <td>42.13</td>\n",
       "      <td>1543600800</td>\n",
       "      <td>26.20</td>\n",
       "      <td>1543575600</td>\n",
       "      <td>40.95</td>\n",
       "      <td>1543608000</td>\n",
       "      <td>17.087683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     timestamp  hour  day  month  \\\n",
       "0  d134c416-199e-485a-a1bb-ce4a9c21881c  1.545123e+09     8   18     12   \n",
       "1  379e231f-6c29-4223-90c9-35ce5aa294c9  1.543528e+09    21   29     11   \n",
       "2  c53e90ed-eeb4-4a64-8290-67d2c83302a2  1.543282e+09     1   27     11   \n",
       "3  6542a54b-836e-4960-925e-0dabf6c1af92  1.543832e+09    10    3     12   \n",
       "4  25caa475-c3da-4770-a729-9625c192827b  1.543621e+09    23   30     11   \n",
       "\n",
       "              datetime          timezone                   source  \\\n",
       "0  2018-12-18 08:55:08  America/New_York                North End   \n",
       "1  2018-11-29 21:48:07  America/New_York            South Station   \n",
       "2  2018-11-27 01:30:22  America/New_York  Northeastern University   \n",
       "3  2018-12-03 10:08:00  America/New_York                North End   \n",
       "4  2018-11-30 23:33:01  America/New_York       Financial District   \n",
       "\n",
       "          destination cab_type  ... uvIndexTime temperatureMin  \\\n",
       "0       North Station     Lyft  ...  1545152400          23.23   \n",
       "1  Financial District     Lyft  ...  1543510800          35.12   \n",
       "2         Beacon Hill     Uber  ...  1543251600          40.49   \n",
       "3       North Station     Uber  ...  1543852800          42.96   \n",
       "4   Boston University     Uber  ...  1543593600          28.90   \n",
       "\n",
       "   temperatureMinTime  temperatureMax  temperatureMaxTime  \\\n",
       "0          1545192000           38.11          1545109200   \n",
       "1          1543550400           44.76          1543510800   \n",
       "2          1543233600           47.30          1543251600   \n",
       "3          1543896000           57.87          1543852800   \n",
       "4          1543579200           42.13          1543600800   \n",
       "\n",
       "   apparentTemperatureMin  apparentTemperatureMinTime  apparentTemperatureMax  \\\n",
       "0                   11.86                  1545134400                   31.96   \n",
       "1                   30.85                  1543550400                   38.44   \n",
       "2                   36.20                  1543291200                   43.92   \n",
       "3                   39.41                  1543896000                   57.20   \n",
       "4                   26.20                  1543575600                   40.95   \n",
       "\n",
       "   apparentTemperatureMaxTime eta_minutes  \n",
       "0                  1545109200    6.628483  \n",
       "1                  1543510800    3.833850  \n",
       "2                  1543251600   12.323333  \n",
       "3                  1543852800    6.628483  \n",
       "4                  1543608000   17.087683  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['name'] == 'Taxi', 'price'] = train_df.loc[train_df['name'] == 'Taxi'].apply(\n",
    "    lambda row: taxi_price_calculator(row['distance'], row['eta_minutes']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                             0\n",
       "timestamp                      0\n",
       "hour                           0\n",
       "day                            0\n",
       "month                          0\n",
       "datetime                       0\n",
       "timezone                       0\n",
       "source                         0\n",
       "destination                    0\n",
       "cab_type                       0\n",
       "product_id                     0\n",
       "name                           0\n",
       "price                          0\n",
       "distance                       0\n",
       "surge_multiplier               0\n",
       "latitude                       0\n",
       "longitude                      0\n",
       "temperature                    0\n",
       "apparentTemperature            0\n",
       "short_summary                  0\n",
       "long_summary                   0\n",
       "precipIntensity                0\n",
       "precipProbability              0\n",
       "humidity                       0\n",
       "windSpeed                      0\n",
       "windGust                       0\n",
       "windGustTime                   0\n",
       "visibility                     0\n",
       "temperatureHigh                0\n",
       "temperatureHighTime            0\n",
       "temperatureLow                 0\n",
       "temperatureLowTime             0\n",
       "apparentTemperatureHigh        0\n",
       "apparentTemperatureHighTime    0\n",
       "apparentTemperatureLow         0\n",
       "apparentTemperatureLowTime     0\n",
       "icon                           0\n",
       "dewPoint                       0\n",
       "pressure                       0\n",
       "windBearing                    0\n",
       "cloudCover                     0\n",
       "uvIndex                        0\n",
       "visibility.1                   0\n",
       "ozone                          0\n",
       "sunriseTime                    0\n",
       "sunsetTime                     0\n",
       "moonPhase                      0\n",
       "precipIntensityMax             0\n",
       "uvIndexTime                    0\n",
       "temperatureMin                 0\n",
       "temperatureMinTime             0\n",
       "temperatureMax                 0\n",
       "temperatureMaxTime             0\n",
       "apparentTemperatureMin         0\n",
       "apparentTemperatureMinTime     0\n",
       "apparentTemperatureMax         0\n",
       "apparentTemperatureMaxTime     0\n",
       "eta_minutes                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi = train_df[train_df[\"name\" ]== \"Taxi\"]\n",
    "df_taxi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('eta_minutes', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in 'price' after imputing: 0.00%\n"
     ]
    }
   ],
   "source": [
    "df_na = train_df[train_df['price'].isna()]\n",
    "missing_percentage_after_imputing =train_df['price'].isna().sum() / len(train_df) * 100\n",
    "print(f\"Percentage of missing values in 'price' after imputing: {missing_percentage_after_imputing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Uneeded Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns that are irrelevant to the modeling because they are just unique identifiers:\n",
    "- `id`\n",
    "- `product_id`\n",
    "\n",
    "In addition, we also dropped `timezone` since all data is within the same timezone.\n",
    "\n",
    "`datetime` and`timestamp` were dropped to reduce redundacy with other time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'timestamp', 'timezone', 'product_id', 'datetime']\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "val_df = val_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hour', 'day', 'month', 'source', 'destination', 'cab_type', 'name',\n",
       "       'price', 'distance', 'surge_multiplier', 'latitude', 'longitude',\n",
       "       'temperature', 'apparentTemperature', 'short_summary', 'long_summary',\n",
       "       'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',\n",
       "       'windGust', 'windGustTime', 'visibility', 'temperatureHigh',\n",
       "       'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',\n",
       "       'apparentTemperatureHigh', 'apparentTemperatureHighTime',\n",
       "       'apparentTemperatureLow', 'apparentTemperatureLowTime', 'icon',\n",
       "       'dewPoint', 'pressure', 'windBearing', 'cloudCover', 'uvIndex',\n",
       "       'visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',\n",
       "       'precipIntensityMax', 'uvIndexTime', 'temperatureMin',\n",
       "       'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',\n",
       "       'apparentTemperatureMin', 'apparentTemperatureMinTime',\n",
       "       'apparentTemperatureMax', 'apparentTemperatureMaxTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Columns:\n",
      "['source', 'destination', 'cab_type', 'name', 'short_summary', 'long_summary', 'icon']\n"
     ]
    }
   ],
   "source": [
    "object_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Display the object columns\n",
    "print(\"Object Columns:\")\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables Encoding\n",
    "### Encoding the `short_summary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Overcast ', ' Clear ', ' Mostly Cloudy ', ' Light Rain ',\n",
       "       ' Partly Cloudy ', ' Possible Drizzle ', ' Rain ', ' Foggy ',\n",
       "       ' Drizzle '], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['short_summary'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The short_summary variable will be divided into binary categories:\n",
    "\n",
    "- Mostly Cloudy\n",
    "- Rain\n",
    "- Clear\n",
    "- Partly Cloudy\n",
    "- Overcast\n",
    "- Light Rain\n",
    "- Foggy\n",
    "- Possible Drizzle\n",
    "- Drizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['short_summary'] = train_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "val_df['short_summary'] = val_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "test_df['short_summary'] = test_df['short_summary'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'short_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['short_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['short_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['short_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['short_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'short_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `long_summary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Mostly cloudy throughout the day. ',\n",
       "       ' Partly cloudy throughout the day. ',\n",
       "       ' Rain throughout the day. ',\n",
       "       ' Light rain in the morning and overnight. ',\n",
       "       ' Light rain in the morning. ', ' Light rain until evening. ',\n",
       "       ' Rain until morning, starting again in the evening. ',\n",
       "       ' Foggy in the morning. ', ' Overcast throughout the day. ',\n",
       "       ' Possible drizzle in the morning. ',\n",
       "       ' Rain in the morning and afternoon. '], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['long_summary'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long_summary variable will be divided into binary categories:\n",
    "\n",
    "- Rain throughout the day\n",
    "- Rain until morning, starting again in the evening\n",
    "- Light rain in the morning\n",
    "- Partly cloudy thoughout the day\n",
    "- Light rain in the morning and overnight\n",
    "- Light rain until evening\n",
    "- Foggy in the morning\n",
    "- Overcast throughout the day\n",
    "- Possible drizzle in the morning\n",
    "- Rain in the morning and afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['long_summary'] = train_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "val_df['long_summary'] = val_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "test_df['long_summary'] = test_df['long_summary'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['long_summary']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['long_summary']])\n",
    "test_encoded_summary = encoder.transform(test_df[['long_summary']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['long_summary']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the icon variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' cloudy ', ' clear-night ', ' partly-cloudy-day ', ' rain ',\n",
       "       ' partly-cloudy-night ', ' clear-day ', ' fog '], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['icon'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for the icon feature will be:\n",
    "\n",
    "- partly cloudy night\n",
    "- rain\n",
    "- clear night\n",
    "- cloudy\n",
    "- fog\n",
    "- clear-day\n",
    "- partly cloudy day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading/trailing spaces and replace blank spaces with underscores in the icon column\n",
    "train_df['icon'] = train_df['icon'].str.strip().str.replace(' ', '_')\n",
    "val_df['icon'] = val_df['icon'].str.strip().str.replace(' ', '_')\n",
    "test_df['icon'] = test_df['icon'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'long_summary' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['icon']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['icon']])\n",
    "test_encoded_summary = encoder.transform(test_df[['icon']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['icon']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'long_summary' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding `source` and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for source:\n",
      "['North End' 'Beacon Hill' 'North Station' 'Boston University'\n",
      " 'South Station' 'Fenway' 'Theatre District' 'West End' 'Back Bay'\n",
      " 'Northeastern University' 'Haymarket Square' 'Financial District']\n",
      "Unique values for destination:\n",
      "['West End' 'South Station' 'Fenway' 'Beacon Hill' 'North Station'\n",
      " 'Back Bay' 'North End' 'Northeastern University' 'Financial District'\n",
      " 'Theatre District' 'Boston University' 'Haymarket Square']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for source:\")\n",
    "print(df['source'].unique())\n",
    "\n",
    "print(\"Unique values for destination:\")\n",
    "print(df['destination'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot-encoded categories for these two features will be:\n",
    "\n",
    "- Haymarket Square\n",
    "- Back Bay\n",
    "- North End\n",
    "- North Station\n",
    "- Beacon Hill\n",
    "- Boston University\n",
    "- Fenway\n",
    "- South Station\n",
    "- Theatre District\n",
    "- West End\n",
    "- Financial District\n",
    "- Northeastern University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['source'] = train_df['source'].str.strip().str.replace(' ', '_')\n",
    "val_df['source'] = val_df['source'].str.strip().str.replace(' ', '_')\n",
    "test_df['source'] = test_df['source'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'source' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['source']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['source']])\n",
    "test_encoded_summary = encoder.transform(test_df[['source']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['source']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'source' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the categories from the fitted OneHotEncoder for 'source'\n",
    "source_categories = encoder.categories_[0]\n",
    "\n",
    "# Initialize a new OneHotEncoder for 'destination'\n",
    "destination_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Strip leading/trailing spaces and replace blank spaces with underscores in the destination column\n",
    "train_df['destination'] = train_df['destination'].str.strip().str.replace(' ', '_')\n",
    "val_df['destination'] = val_df['destination'].str.strip().str.replace(' ', '_')\n",
    "test_df['destination'] = test_df['destination'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Fit the encoder on the training data 'destination' column\n",
    "train_encoded_destination = destination_encoder.fit_transform(train_df[['destination']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_destination = destination_encoder.transform(val_df[['destination']])\n",
    "test_encoded_destination = destination_encoder.transform(test_df[['destination']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_destination_df = pd.DataFrame(train_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "val_encoded_destination_df = pd.DataFrame(val_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "test_encoded_destination_df = pd.DataFrame(test_encoded_destination, columns=destination_encoder.get_feature_names_out(['destination']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'destination' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_destination_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_destination_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_destination_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(\"Train DataFrame:\")\n",
    "# print(train_df.head())\n",
    "# print(\"\\nValidation DataFrame:\")\n",
    "# print(val_df.head())\n",
    "# print(\"\\nTest DataFrame:\")\n",
    "# print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `name` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UberXL', 'Black', 'UberX', 'WAV', 'Black SUV', 'UberPool', 'Taxi',\n",
       "       'Shared', 'Lux', 'Lyft', 'Lux Black XL', 'Lyft XL', 'Lux Black'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['name'] = train_df['name'].str.strip().str.replace(' ', '_')\n",
    "val_df['name'] = val_df['name'].str.strip().str.replace(' ', '_')\n",
    "test_df['name'] = test_df['name'].str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data 'name' column\n",
    "train_encoded_summary = encoder.fit_transform(train_df[['name']])\n",
    "\n",
    "# Apply the encoder to validation and test sets using the trained categories from the training data\n",
    "val_encoded_summary = encoder.transform(val_df[['name']])\n",
    "test_encoded_summary = encoder.transform(test_df[['name']])\n",
    "\n",
    "# Convert the encoded arrays back to pandas DataFrames with appropriate column names\n",
    "train_encoded_summary_df = pd.DataFrame(train_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "val_encoded_summary_df = pd.DataFrame(val_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "test_encoded_summary_df = pd.DataFrame(test_encoded_summary, columns=encoder.get_feature_names_out(['name']))\n",
    "\n",
    "# Concatenate the one-hot encoded 'name' columns back to the respective datasets\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_encoded_summary_df], axis=1)\n",
    "val_df = pd.concat([val_df.reset_index(drop=True), val_encoded_summary_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_encoded_summary_df], axis=1)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the `cab_type` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Uber', 'Lyft'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cab_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cab_type'] = train_df['cab_type'].str.strip()\n",
    "val_df['cab_type'] = val_df['cab_type'].str.strip()\n",
    "test_df['cab_type'] = test_df['cab_type'].str.strip()\n",
    "\n",
    "# Ensure there are no leading or trailing spaces in the 'cab_type' column\n",
    "train_df['cab_type'] = train_df['cab_type'].str.strip()\n",
    "val_df['cab_type'] = val_df['cab_type'].str.strip()\n",
    "test_df['cab_type'] = test_df['cab_type'].str.strip()\n",
    "\n",
    "# Create dummy variables for the 'cab_type' column\n",
    "train_df = pd.get_dummies(train_df, columns=['cab_type'], drop_first=True)\n",
    "val_df = pd.get_dummies(val_df, columns=['cab_type'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['cab_type'], drop_first=True)\n",
    "\n",
    "# # Check the resulting dataframe\n",
    "# print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_to_drop = ['source',\n",
    " 'destination',\n",
    " 'name',\n",
    " 'short_summary',\n",
    " 'long_summary',\n",
    " 'icon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = cat_columns_to_drop)\n",
    "val_df = val_df.drop(columns = cat_columns_to_drop)\n",
    "test_df = test_df.drop(columns = cat_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Information using .info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 485150 entries, 0 to 485149\n",
      "Columns: 110 entries, hour to cab_type_Uber\n",
      "dtypes: bool(1), float64(92), int64(17)\n",
      "memory usage: 403.9 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataFrame Information using .info():\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[['price']]\n",
    "y_val = val_df[['price']]\n",
    "y_test = test_df[['price']]\n",
    "\n",
    "X_train = train_df.drop('price', axis = 1)\n",
    "X_val = val_df.drop('price', axis = 1)\n",
    "X_test = test_df.drop('price', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaled DataFrames as Parquet files\n",
    "processed_data_folder = os.path.join(data_folder, \"processed\")\n",
    "\n",
    "X_train_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_train_scaled.parquet'), index=False)\n",
    "X_val_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_val_scaled.parquet'), index=False)\n",
    "X_test_scaled_df.to_parquet(os.path.join(processed_data_folder, 'X_test_scaled.parquet'), index=False)\n",
    "\n",
    "# Save the target variables (if needed)\n",
    "y_train.to_parquet(os.path.join(processed_data_folder, 'y_train.parquet'), index=False)\n",
    "y_val.to_parquet(os.path.join(processed_data_folder, 'y_val.parquet'), index=False)\n",
    "y_test.to_parquet(os.path.join(processed_data_folder, 'y_test.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA, you can adjust n_components as needed (for example, n_components=0.95 for 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit PCA on the scaled training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the validation and test sets using the same PCA\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Convert the PCA results back to DataFrames for easier handling\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "X_val_pca_df = pd.DataFrame(X_val_pca)\n",
    "X_test_pca_df = pd.DataFrame(X_test_pca)\n",
    "\n",
    "# Optional: Rename the columns for clarity\n",
    "X_train_pca_df.columns = [f'PC{i+1}' for i in range(X_train_pca_df.shape[1])]\n",
    "X_val_pca_df.columns = [f'PC{i+1}' for i in range(X_val_pca_df.shape[1])]\n",
    "X_test_pca_df.columns = [f'PC{i+1}' for i in range(X_test_pca_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(485150, 56)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduces the df to 56 components, which is very useful for models like linear regression which would suffer from high-dimentional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_path = os.path.join(processed_data_folder, 'X_train_pca.parquet')\n",
    "val_pca_path = os.path.join(processed_data_folder, 'X_val_pca.parquet')\n",
    "test_pca_path = os.path.join(processed_data_folder, 'X_test_pca.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA DataFrames as Parquet files\n",
    "X_train_pca_df.to_parquet(train_pca_path, index=False)\n",
    "X_val_pca_df.to_parquet(val_pca_path, index=False)\n",
    "X_test_pca_df.to_parquet(test_pca_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
